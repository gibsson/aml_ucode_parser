commit 5c56a0e59ca9ac74ea1de3bfde00bf9a3afb3c90
Author: Yao.Liu <yao.liu@amlogic.com>
Date:   Thu Feb 21 09:01:39 2019 -0500

    encoder: H264 and H265 support dma buf [1/1]
    
    PD#SWPL-4905
    
    Problem:
    In order to reduce the memory bandwidth
    signficantly by avoiding copying full
    resolution images, h264/h265 support
    using dma buffer as input buffer
    
    Solution:
    Support dma buffer
    
    Verify:
    verify by testAPI from google-newman branch
    
    Change-Id: I9d43b81d5adb9b09f8973604709a3a8a720423c9
    Signed-off-by: Yao.Liu <yao.liu@amlogic.com>

diff --git a/drivers/frame_sink/encoder/h264/encoder.c b/drivers/frame_sink/encoder/h264/encoder.c
index fadff43..2295bc3 100644
--- a/drivers/frame_sink/encoder/h264/encoder.c
+++ b/drivers/frame_sink/encoder/h264/encoder.c
@@ -49,6 +49,8 @@
 #include "../../../stream_input/amports/amports_priv.h"
 #include "../../../frame_provider/decoder/utils/firmware.h"
 #include <linux/of_reserved_mem.h>
+
+
 #ifdef CONFIG_AM_JPEG_ENCODER
 #include "jpegenc.h"
 #endif
@@ -419,6 +421,8 @@ const char *ucode_name[] = {
 
 static void dma_flush(u32 buf_start, u32 buf_size);
 static void cache_flush(u32 buf_start, u32 buf_size);
+static int enc_dma_buf_get_phys(struct enc_dma_cfg *cfg, unsigned long *addr);
+static void enc_dma_buf_unmap(struct enc_dma_cfg *cfg);
 
 static const char *select_ucode(u32 ucode_index)
 {
@@ -1204,6 +1208,9 @@ static s32 set_input_format(struct encode_wq_s *wq,
 	u32 picsize_x, picsize_y, src_addr;
 	u32 canvas_w = 0;
 	u32 input = request->src;
+	u32 input_y = 0;
+	u32 input_u = 0;
+	u32 input_v = 0;
 	u8 ifmt_extra = 0;
 
 	if ((request->fmt == FMT_RGB565) || (request->fmt >= MAX_FRAME_FMT))
@@ -1213,7 +1220,8 @@ static s32 set_input_format(struct encode_wq_s *wq,
 	picsize_y = ((wq->pic.encoder_height + 15) >> 4) << 4;
 	oformat = 0;
 	if ((request->type == LOCAL_BUFF)
-		|| (request->type == PHYSICAL_BUFF)) {
+		|| (request->type == PHYSICAL_BUFF)
+		|| (request->type == DMA_BUFF)) {
 		if ((request->type == LOCAL_BUFF) &&
 			(request->flush_flag & AMVENC_FLUSH_FLAG_INPUT))
 			dma_flush(wq->mem.dct_buff_start_addr,
@@ -1222,6 +1230,36 @@ static s32 set_input_format(struct encode_wq_s *wq,
 			input = wq->mem.dct_buff_start_addr;
 			src_addr =
 				wq->mem.dct_buff_start_addr;
+		} else if (request->type == DMA_BUFF) {
+			if (request->plane_num == 3) {
+				input_y = (unsigned long)request->dma_cfg[0].paddr;
+				input_u = (unsigned long)request->dma_cfg[1].paddr;
+				input_v = (unsigned long)request->dma_cfg[2].paddr;
+			} else if (request->plane_num == 2) {
+				input_y = (unsigned long)request->dma_cfg[0].paddr;
+				input_u = (unsigned long)request->dma_cfg[1].paddr;
+				input_v = input_u;
+			} else if (request->plane_num == 1) {
+				input_y = (unsigned long)request->dma_cfg[0].paddr;
+				if (request->fmt == FMT_NV21
+					|| request->fmt == FMT_NV12) {
+					input_u = input_y + picsize_x * picsize_y;
+					input_v = input_u;
+				}
+				if (request->fmt == FMT_YUV420) {
+					input_u = input_y + picsize_x * picsize_y;
+					input_v = input_u + picsize_x * picsize_y  / 4;
+				}
+			}
+			src_addr = input_y;
+			picsize_y = wq->pic.encoder_height;
+			enc_pr(LOG_INFO, "dma addr[0x%lx 0x%lx 0x%lx 0x%lx 0x%lx 0x%lx]\n",
+				(unsigned long)request->dma_cfg[0].vaddr,
+				(unsigned long)request->dma_cfg[0].paddr,
+				(unsigned long)request->dma_cfg[1].vaddr,
+				(unsigned long)request->dma_cfg[1].paddr,
+				(unsigned long)request->dma_cfg[2].vaddr,
+				(unsigned long)request->dma_cfg[2].paddr);
 		} else {
 			src_addr = input;
 			picsize_y = wq->pic.encoder_height;
@@ -1291,36 +1329,68 @@ static s32 set_input_format(struct encode_wq_s *wq,
 			|| (request->fmt == FMT_NV12)) {
 			canvas_w = ((wq->pic.encoder_width + 31) >> 5) << 5;
 			iformat = (request->fmt == FMT_NV21) ? 2 : 3;
-			canvas_config(ENC_CANVAS_OFFSET + 6,
-				input,
-				canvas_w, picsize_y,
-				CANVAS_ADDR_NOWRAP,
-				CANVAS_BLKMODE_LINEAR);
-			canvas_config(ENC_CANVAS_OFFSET + 7,
-				input + canvas_w * picsize_y,
-				canvas_w, picsize_y / 2,
-				CANVAS_ADDR_NOWRAP,
-				CANVAS_BLKMODE_LINEAR);
+			if (request->type == DMA_BUFF) {
+				canvas_config(ENC_CANVAS_OFFSET + 6,
+					input_y,
+					canvas_w, picsize_y,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+				canvas_config(ENC_CANVAS_OFFSET + 7,
+					input_u,
+					canvas_w, picsize_y / 2,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+			} else {
+				canvas_config(ENC_CANVAS_OFFSET + 6,
+					input,
+					canvas_w, picsize_y,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+				canvas_config(ENC_CANVAS_OFFSET + 7,
+					input + canvas_w * picsize_y,
+					canvas_w, picsize_y / 2,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+			}
 			input = ((ENC_CANVAS_OFFSET + 7) << 8) |
 				(ENC_CANVAS_OFFSET + 6);
 		} else if (request->fmt == FMT_YUV420) {
 			iformat = 4;
 			canvas_w = ((wq->pic.encoder_width + 63) >> 6) << 6;
-			canvas_config(ENC_CANVAS_OFFSET + 6,
-				input,
-				canvas_w, picsize_y,
-				CANVAS_ADDR_NOWRAP,
-				CANVAS_BLKMODE_LINEAR);
-			canvas_config(ENC_CANVAS_OFFSET + 7,
-				input + canvas_w * picsize_y,
-				canvas_w / 2, picsize_y / 2,
-				CANVAS_ADDR_NOWRAP,
-				CANVAS_BLKMODE_LINEAR);
-			canvas_config(ENC_CANVAS_OFFSET + 8,
-				input + canvas_w * picsize_y * 5 / 4,
-				canvas_w / 2, picsize_y / 2,
-				CANVAS_ADDR_NOWRAP,
-				CANVAS_BLKMODE_LINEAR);
+			if (request->type == DMA_BUFF) {
+				canvas_config(ENC_CANVAS_OFFSET + 6,
+					input_y,
+					canvas_w, picsize_y,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+				canvas_config(ENC_CANVAS_OFFSET + 7,
+					input_u,
+					canvas_w / 2, picsize_y / 2,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+				canvas_config(ENC_CANVAS_OFFSET + 8,
+					input_v,
+					canvas_w / 2, picsize_y / 2,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+			} else {
+				canvas_config(ENC_CANVAS_OFFSET + 6,
+					input,
+					canvas_w, picsize_y,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+				canvas_config(ENC_CANVAS_OFFSET + 7,
+					input + canvas_w * picsize_y,
+					canvas_w / 2, picsize_y / 2,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+				canvas_config(ENC_CANVAS_OFFSET + 8,
+					input + canvas_w * picsize_y * 5 / 4,
+					canvas_w / 2, picsize_y / 2,
+					CANVAS_ADDR_NOWRAP,
+					CANVAS_BLKMODE_LINEAR);
+
+			}
 			input = ((ENC_CANVAS_OFFSET + 8) << 16) |
 				((ENC_CANVAS_OFFSET + 7) << 8) |
 				(ENC_CANVAS_OFFSET + 6);
@@ -2555,6 +2625,10 @@ static s32 convert_request(struct encode_wq_s *wq, u32 *cmd_info)
 	u8 *ptr;
 	u32 data_offset;
 	u32 cmd = cmd_info[0];
+	unsigned long paddr = 0;
+	struct enc_dma_cfg *cfg = NULL;
+	s32 ret = 0;
+	struct platform_device *pdev;
 
 	if (!wq)
 		return -1;
@@ -2653,6 +2727,44 @@ static s32 convert_request(struct encode_wq_s *wq, u32 *cmd_info)
 		wq->cbr_info.short_shift = CBR_SHORT_SHIFT;
 		wq->cbr_info.long_mb_num = CBR_LONG_MB_NUM;
 #endif
+		data_offset = 17 +
+				(sizeof(wq->quant_tbl_i4)
+				+ sizeof(wq->quant_tbl_i16)
+				+ sizeof(wq->quant_tbl_me)) / 4 + 7;
+
+		if (wq->request.type == DMA_BUFF) {
+			wq->request.plane_num = cmd_info[data_offset++];
+			enc_pr(LOG_INFO, "wq->request.plane_num %d\n",
+				wq->request.plane_num);
+			if (wq->request.fmt == FMT_NV12 ||
+				wq->request.fmt == FMT_NV21 ||
+				wq->request.fmt == FMT_YUV420) {
+				for (i = 0; i < wq->request.plane_num; i++) {
+					cfg = &wq->request.dma_cfg[i];
+					cfg->dir = DMA_TO_DEVICE;
+					cfg->fd = cmd_info[data_offset++];
+					pdev = encode_manager.this_pdev;
+					cfg->dev = &(pdev->dev);
+
+					ret = enc_dma_buf_get_phys(cfg, &paddr);
+					if (ret < 0) {
+						enc_pr(LOG_ERROR,
+							"import fd %d failed\n",
+							cfg->fd);
+						cfg->paddr = NULL;
+						cfg->vaddr = NULL;
+						return -1;
+					}
+					cfg->paddr = (void *)paddr;
+					enc_pr(LOG_INFO, "vaddr %p\n",
+						cfg->vaddr);
+				}
+			} else {
+				enc_pr(LOG_ERROR, "error fmt = %d\n",
+					wq->request.fmt);
+			}
+		}
+
 	} else {
 		enc_pr(LOG_ERROR, "error cmd = %d, wq: %p.\n",
 			cmd, (void *)wq);
@@ -2707,6 +2819,7 @@ void amvenc_avc_start_cmd(struct encode_wq_s *wq,
 	if ((request->cmd == ENCODER_IDR) ||
 		(request->cmd == ENCODER_NON_IDR))
 		set_input_format(wq, request);
+
 	if (request->cmd == ENCODER_IDR)
 		ie_me_mb_type = HENC_MB_Type_I4MB;
 	else if (request->cmd == ENCODER_NON_IDR)
@@ -2744,7 +2857,7 @@ void amvenc_avc_start_cmd(struct encode_wq_s *wq,
 
 	if (reload_flag)
 		amvenc_start();
-	enc_pr(LOG_ALL, "amvenc_avc_start cmd, wq:%p.\n", (void *)wq);
+	enc_pr(LOG_ALL, "amvenc_avc_start cmd out, request:%p.\n", (void*)request);
 }
 
 static void dma_flush(u32 buf_start, u32 buf_size)
@@ -2977,7 +3090,7 @@ static s32 amvenc_avc_open(struct inode *inode, struct file *file)
 	}
 
 	memcpy(&wq->mem.bufspec, &amvenc_buffspec[0],
-	       sizeof(struct BuffInfo_s));
+		sizeof(struct BuffInfo_s));
 
 	enc_pr(LOG_DEBUG,
 		"amvenc_avc  memory config success, buff start:0x%x, size is 0x%x, wq:%p.\n",
@@ -3003,7 +3116,7 @@ static long amvenc_avc_ioctl(struct file *file, u32 cmd, ulong arg)
 	long r = 0;
 	u32 amrisc_cmd = 0;
 	struct encode_wq_s *wq = (struct encode_wq_s *)file->private_data;
-#define MAX_ADDR_INFO_SIZE 50
+#define MAX_ADDR_INFO_SIZE 52
 	u32 addr_info[MAX_ADDR_INFO_SIZE + 4];
 	ulong argV;
 	u32 buf_start;
@@ -3264,6 +3377,9 @@ static s32 encode_process_request(struct encode_manager_s *manager,
 	u32 flush_size = ((wq->pic.encoder_width + 31) >> 5 << 5) *
 		((wq->pic.encoder_height + 15) >> 4 << 4) * 3 / 2;
 
+	struct enc_dma_cfg *cfg = NULL;
+	int i = 0;
+
 #ifdef H264_ENC_CBR
 	if (request->cmd == ENCODER_IDR || request->cmd == ENCODER_NON_IDR) {
 		if (request->flush_flag & AMVENC_FLUSH_FLAG_CBR
@@ -3362,6 +3478,13 @@ Again:
 				READ_HREG(DEBUG_REG));
 			amvenc_avc_light_reset(wq, 30);
 		}
+		for (i = 0; i < request->plane_num; i++) {
+			cfg = &request->dma_cfg[i];
+			enc_pr(LOG_INFO, "request vaddr %p, paddr %p\n",
+				cfg->vaddr, cfg->paddr);
+			if (cfg->fd >= 0 && cfg->vaddr != NULL)
+				enc_dma_buf_unmap(cfg);
+		}
 	}
 	atomic_inc(&wq->request_ready);
 	wake_up_interruptible(&wq->request_complete);
@@ -3403,7 +3526,14 @@ s32 encode_wq_add_request(struct encode_wq_s *wq)
 		goto error;
 
 	memcpy(&pitem->request, &wq->request, sizeof(struct encode_request_s));
+
+	enc_pr(LOG_INFO, "new work request %p, vaddr %p, paddr %p\n", &pitem->request,
+		pitem->request.dma_cfg[0].vaddr,pitem->request.dma_cfg[0].paddr);
+
 	memset(&wq->request, 0, sizeof(struct encode_request_s));
+	wq->request.dma_cfg[0].fd = -1;
+	wq->request.dma_cfg[1].fd = -1;
+	wq->request.dma_cfg[2].fd = -1;
 	wq->hw_status = 0;
 	wq->output_size = 0;
 	pitem->request.parent = wq;
@@ -4154,6 +4284,139 @@ static s32 __init avc_mem_setup(struct reserved_mem *rmem)
 	return 0;
 }
 
+static int enc_dma_buf_map(struct enc_dma_cfg *cfg)
+{
+	long ret = -1;
+	int fd = -1;
+	struct dma_buf *dbuf = NULL;
+	struct dma_buf_attachment *d_att = NULL;
+	struct sg_table *sg = NULL;
+	void *vaddr = NULL;
+	struct device *dev = NULL;
+	enum dma_data_direction dir;
+
+	if (cfg == NULL || (cfg->fd < 0) || cfg->dev == NULL) {
+		enc_pr(LOG_ERROR, "error input param\n");
+		return -EINVAL;
+	}
+	enc_pr(LOG_INFO, "enc_dma_buf_map, fd %d\n", cfg->fd);
+
+	fd = cfg->fd;
+	dev = cfg->dev;
+	dir = cfg->dir;
+	enc_pr(LOG_INFO, "enc_dma_buffer_map fd %d\n", fd);
+
+	dbuf = dma_buf_get(fd);
+	if (dbuf == NULL) {
+		enc_pr(LOG_ERROR, "failed to get dma buffer,fd %d\n",fd);
+		return -EINVAL;
+	}
+
+	d_att = dma_buf_attach(dbuf, dev);
+	if (d_att == NULL) {
+		enc_pr(LOG_ERROR, "failed to set dma attach\n");
+		goto attach_err;
+	}
+
+	sg = dma_buf_map_attachment(d_att, dir);
+	if (sg == NULL) {
+		enc_pr(LOG_ERROR, "failed to get dma sg\n");
+		goto map_attach_err;
+	}
+
+	ret = dma_buf_begin_cpu_access(dbuf, dir);
+	if (ret != 0) {
+		enc_pr(LOG_ERROR, "failed to access dma buff\n");
+		goto access_err;
+	}
+
+	vaddr = dma_buf_vmap(dbuf);
+	if (vaddr == NULL) {
+		enc_pr(LOG_ERROR, "failed to vmap dma buf\n");
+		goto vmap_err;
+	}
+	cfg->dbuf = dbuf;
+	cfg->attach = d_att;
+	cfg->vaddr = vaddr;
+	cfg->sg = sg;
+
+	return ret;
+
+vmap_err:
+	dma_buf_end_cpu_access(dbuf, dir);
+
+access_err:
+	dma_buf_unmap_attachment(d_att, sg, dir);
+
+map_attach_err:
+	dma_buf_detach(dbuf, d_att);
+
+attach_err:
+	dma_buf_put(dbuf);
+
+	return ret;
+}
+
+static int enc_dma_buf_get_phys(struct enc_dma_cfg *cfg, unsigned long *addr)
+{
+	struct sg_table *sg_table;
+	struct page *page;
+	int ret;
+	enc_pr(LOG_INFO, "enc_dma_buf_get_phys in\n");
+
+	ret = enc_dma_buf_map(cfg);
+	if (ret < 0) {
+		enc_pr(LOG_ERROR, "gdc_dma_buf_map failed\n");
+		return ret;
+	}
+	if (cfg->sg) {
+		sg_table = cfg->sg;
+		page = sg_page(sg_table->sgl);
+		*addr = PFN_PHYS(page_to_pfn(page));
+		ret = 0;
+	}
+	enc_pr(LOG_INFO, "enc_dma_buf_get_phys 0x%lx\n", *addr);
+	return ret;
+}
+
+static void enc_dma_buf_unmap(struct enc_dma_cfg *cfg)
+{
+	int fd = -1;
+	struct dma_buf *dbuf = NULL;
+	struct dma_buf_attachment *d_att = NULL;
+	struct sg_table *sg = NULL;
+	void *vaddr = NULL;
+	struct device *dev = NULL;
+	enum dma_data_direction dir;
+
+	if (cfg == NULL || (cfg->fd < 0) || cfg->dev == NULL
+			|| cfg->dbuf == NULL || cfg->vaddr == NULL
+			|| cfg->attach == NULL || cfg->sg == NULL) {
+		enc_pr(LOG_ERROR, "Error input param\n");
+		return;
+	}
+
+	fd = cfg->fd;
+	dev = cfg->dev;
+	dir = cfg->dir;
+	dbuf = cfg->dbuf;
+	vaddr = cfg->vaddr;
+	d_att = cfg->attach;
+	sg = cfg->sg;
+
+	dma_buf_vunmap(dbuf, vaddr);
+
+	dma_buf_end_cpu_access(dbuf, dir);
+
+	dma_buf_unmap_attachment(d_att, sg, dir);
+
+	dma_buf_detach(dbuf, d_att);
+
+	dma_buf_put(dbuf);
+	enc_pr(LOG_DEBUG, "enc_dma_buffer_unmap vaddr %p\n",(unsigned *)vaddr);
+}
+
+
 module_param(fixed_slice_cfg, uint, 0664);
 MODULE_PARM_DESC(fixed_slice_cfg, "\n fixed_slice_cfg\n");
 
diff --git a/drivers/frame_sink/encoder/h264/encoder.h b/drivers/frame_sink/encoder/h264/encoder.h
index a5e0cee..39a3c1e 100644
--- a/drivers/frame_sink/encoder/h264/encoder.h
+++ b/drivers/frame_sink/encoder/h264/encoder.h
@@ -31,6 +31,8 @@
 #include <linux/amlogic/media/ge2d/ge2d.h>
 #endif
 
+#include <linux/dma-buf.h>
+
 #define AMVENC_DEVINFO_M8 "AML-M8"
 #define AMVENC_DEVINFO_G9 "AML-G9"
 #define AMVENC_DEVINFO_GXBB "AML-GXBB"
@@ -132,6 +134,7 @@ enum amvenc_mem_type_e {
 	LOCAL_BUFF = 0,
 	CANVAS_BUFF,
 	PHYSICAL_BUFF,
+	DMA_BUFF,
 	MAX_BUFF_TYPE
 };
 
@@ -181,13 +184,22 @@ enum amvenc_frame_fmt_e {
 
 struct encode_wq_s;
 
+struct enc_dma_cfg {
+	int fd;
+	void *dev;
+	void *vaddr;
+	void *paddr;
+	struct dma_buf *dbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sg;
+	enum dma_data_direction dir;
+};
+
 struct encode_request_s {
 	u32 quant;
 	u32 cmd;
 	u32 ucode_mode;
-
 	u32 src;
-
 	u32 framesize;
 
 	u32 me_weight;
@@ -208,6 +220,8 @@ struct encode_request_s {
 	enum amvenc_mem_type_e type;
 	enum amvenc_frame_fmt_e fmt;
 	struct encode_wq_s *parent;
+	struct enc_dma_cfg dma_cfg[3];
+	u32 plane_num;
 };
 
 struct encode_queue_item_s {
diff --git a/drivers/frame_sink/encoder/h265/vpu.c b/drivers/frame_sink/encoder/h265/vpu.c
index 3180b49..8775930 100644
--- a/drivers/frame_sink/encoder/h265/vpu.c
+++ b/drivers/frame_sink/encoder/h265/vpu.c
@@ -106,6 +106,11 @@ static struct platform_device *hevc_pdev;
 
 static struct vpu_bit_firmware_info_t s_bit_firmware_info[MAX_NUM_VPU_CORE];
 
+static struct vpu_dma_cfg dma_cfg[3];
+
+static u32 vpu_src_addr_config(struct vpu_dma_buf_info_t);
+static void vpu_dma_buffer_unmap(struct vpu_dma_cfg *cfg);
+
 static void dma_flush(u32 buf_start, u32 buf_size)
 {
 	if (hevc_pdev)
@@ -427,6 +432,10 @@ static s32 vpu_open(struct inode *inode, struct file *filp)
 
 		spin_unlock_irqrestore(&s_vpu_lock, flags);
 	}
+	memset(dma_cfg, 0, sizeof(dma_cfg));
+	dma_cfg[0].fd = -1;
+	dma_cfg[1].fd = -1;
+	dma_cfg[2].fd = -1;
 Err:
 	if (r != 0)
 		s_vpu_drv_context.open_count--;
@@ -1241,10 +1250,50 @@ static long vpu_ioctl(struct file *filp, u32 cmd, ulong arg)
 				"[-]VDI_IOCTL_FLUSH_BUFFER\n");
 		}
 		break;
+	case VDI_IOCTL_CONFIG_DMA:
+		{
+			struct vpu_dma_buf_info_t dma_info;
+
+			enc_pr(LOG_ALL,
+				"[+]VDI_IOCTL_CONFIG_DMA\n");
+			if (copy_from_user(&dma_info,
+				(struct vpu_dma_buf_info_t *)arg,
+				sizeof(struct vpu_dma_buf_info_t)))
+				return -EFAULT;
+
+			if (vpu_src_addr_config(dma_info)) {
+				enc_pr(LOG_ERROR,
+					"src addr config error\n");
+				return -EFAULT;
+			}
+
+			enc_pr(LOG_ALL,
+				"[-]VDI_IOCTL_CONFIG_DMA %d, %d, %d\n",
+				dma_info.fd[0],
+				dma_info.fd[1],
+				dma_info.fd[2]);
+		}
+		break;
+	case VDI_IOCTL_UNMAP_DMA:
+		{
+			enc_pr(LOG_ALL,
+				"[+]VDI_IOCTL_UNMAP_DMA\n");
+
+			vpu_dma_buffer_unmap(&dma_cfg[0]);
+			if (dma_cfg[1].paddr != 0) {
+				vpu_dma_buffer_unmap(&dma_cfg[1]);
+			}
+			if (dma_cfg[2].paddr != 0) {
+				vpu_dma_buffer_unmap(&dma_cfg[2]);
+			}
+			enc_pr(LOG_ALL,
+				"[-]VDI_IOCTL_UNMAP_DMA\n");
+		}
+		break;
 	default:
 		{
 			enc_pr(LOG_ERROR,
-				"No such IOCTL, cmd is %d\n", cmd);
+				"No such IOCTL, cmd is 0x%x\n", cmd);
 			ret = -EFAULT;
 		}
 		break;
@@ -1475,6 +1524,194 @@ static s32 vpu_mmap(struct file *fp, struct vm_area_struct *vm)
 
 	return vpu_map_to_physical_memory(fp, vm);
 }
+static int vpu_dma_buffer_map(struct vpu_dma_cfg *cfg)
+{
+	int ret = -1;
+	int fd = -1;
+	struct dma_buf *dbuf = NULL;
+	struct dma_buf_attachment *d_att = NULL;
+	struct sg_table *sg = NULL;
+	void *vaddr = NULL;
+	struct device *dev = NULL;
+	enum dma_data_direction dir;
+
+	if (cfg == NULL || (cfg->fd < 0) || cfg->dev == NULL) {
+		enc_pr(LOG_ERROR, "error dma param\n");
+		return -EINVAL;
+	}
+	fd = cfg->fd;
+	dev = cfg->dev;
+	dir = cfg->dir;
+
+	dbuf = dma_buf_get(fd);
+	if (dbuf == NULL) {
+		enc_pr(LOG_ERROR, "failed to get dma buffer,fd %d\n",fd);
+		return -EINVAL;
+	}
+
+	d_att = dma_buf_attach(dbuf, dev);
+	if (d_att == NULL) {
+		enc_pr(LOG_ERROR, "failed to set dma attach\n");
+		goto attach_err;
+	}
+
+	sg = dma_buf_map_attachment(d_att, dir);
+	if (sg == NULL) {
+		enc_pr(LOG_ERROR, "failed to get dma sg\n");
+		goto map_attach_err;
+	}
+	cfg->dbuf = dbuf;
+	cfg->attach = d_att;
+	cfg->vaddr = vaddr;
+	cfg->sg = sg;
+
+	return 0;
+
+map_attach_err:
+	dma_buf_detach(dbuf, d_att);
+attach_err:
+	dma_buf_put(dbuf);
+
+	return ret;
+}
+
+static void vpu_dma_buffer_unmap(struct vpu_dma_cfg *cfg)
+{
+	int fd = -1;
+	struct dma_buf *dbuf = NULL;
+	struct dma_buf_attachment *d_att = NULL;
+	struct sg_table *sg = NULL;
+	/*void *vaddr = NULL;*/
+	struct device *dev = NULL;
+	enum dma_data_direction dir;
+
+	if (cfg == NULL || (cfg->fd < 0) || cfg->dev == NULL
+			|| cfg->dbuf == NULL /*|| cfg->vaddr == NULL*/
+			|| cfg->attach == NULL || cfg->sg == NULL) {
+		enc_pr(LOG_ERROR, "unmap: Error dma param\n");
+		return;
+	}
+
+	fd = cfg->fd;
+	dev = cfg->dev;
+	dir = cfg->dir;
+	dbuf = cfg->dbuf;
+	d_att = cfg->attach;
+	sg = cfg->sg;
+
+	dma_buf_unmap_attachment(d_att, sg, dir);
+	dma_buf_detach(dbuf, d_att);
+	dma_buf_put(dbuf);
+
+	enc_pr(LOG_INFO, "vpu_dma_buffer_unmap fd %d\n",fd);
+}
+
+static int vpu_dma_buffer_get_phys(struct vpu_dma_cfg *cfg, unsigned long *addr)
+{
+	struct sg_table *sg_table;
+	struct page *page;
+	int ret;
+
+	ret = vpu_dma_buffer_map(cfg);
+	if (ret < 0) {
+		printk("vpu_dma_buffer_map failed\n");
+		return ret;
+	}
+	if (cfg->sg) {
+		sg_table = cfg->sg;
+		page = sg_page(sg_table->sgl);
+		*addr = PFN_PHYS(page_to_pfn(page));
+		ret = 0;
+	}
+	enc_pr(LOG_INFO,"vpu_dma_buffer_get_phys\n");
+
+	return ret;
+}
+
+static u32 vpu_src_addr_config(struct vpu_dma_buf_info_t info) {
+	unsigned long phy_addr_y = 0;
+	unsigned long phy_addr_u = 0;
+	unsigned long phy_addr_v = 0;
+	unsigned long Ysize = info.width * info.height;
+	unsigned long Usize = Ysize >> 2;
+	s32 ret = 0;
+	u32 core = 0;
+
+	//y
+	dma_cfg[0].dir = DMA_TO_DEVICE;
+	dma_cfg[0].fd = info.fd[0];
+	dma_cfg[0].dev = &(hevc_pdev->dev);
+	ret = vpu_dma_buffer_get_phys(&dma_cfg[0], &phy_addr_y);
+	if (ret < 0) {
+		enc_pr(LOG_ERROR, "import fd %d failed\n", info.fd[0]);
+		return -1;
+	}
+
+	//u
+	if (info.num_planes >=2) {
+		dma_cfg[1].dir = DMA_TO_DEVICE;
+		dma_cfg[1].fd = info.fd[1];
+		dma_cfg[1].dev = &(hevc_pdev->dev);
+		ret = vpu_dma_buffer_get_phys(&dma_cfg[1], &phy_addr_u);
+		if (ret < 0) {
+			enc_pr(LOG_ERROR, "import fd %d failed\n", info.fd[1]);
+			return -1;
+		}
+	}
+
+	//v
+	if (info.num_planes >=3) {
+		dma_cfg[2].dir = DMA_TO_DEVICE;
+		dma_cfg[2].fd = info.fd[2];
+		dma_cfg[2].dev = &(hevc_pdev->dev);
+		ret = vpu_dma_buffer_get_phys(&dma_cfg[2], &phy_addr_v);
+		if (ret < 0) {
+			enc_pr(LOG_ERROR, "import fd %d failed\n", info.fd[2]);
+			return -1;
+		}
+	}
+
+	enc_pr(LOG_INFO, "vpu_src_addr_config phy_addr 0x%lx, 0x%lx, 0x%lx\n",
+		phy_addr_y, phy_addr_u, phy_addr_v);
+
+	dma_cfg[0].paddr = (void *)phy_addr_y;
+	dma_cfg[1].paddr = (void *)phy_addr_u;
+	dma_cfg[2].paddr = (void *)phy_addr_v;
+
+	enc_pr(LOG_INFO, "info.num_planes %d, info.fmt %d\n",
+		info.num_planes, info.fmt);
+
+	WriteVpuRegister(W4_SRC_ADDR_Y, phy_addr_y);
+	if (info.num_planes == 1) {
+		if (info.fmt == AMVENC_YUV420) {
+			WriteVpuRegister(W4_SRC_ADDR_U, phy_addr_y + Ysize);
+			WriteVpuRegister(W4_SRC_ADDR_V, phy_addr_y + Ysize + Usize);
+		} else if (info.fmt == AMVENC_NV12 || info.fmt == AMVENC_NV21 ) {
+			WriteVpuRegister(W4_SRC_ADDR_U, phy_addr_y + Ysize);
+			WriteVpuRegister(W4_SRC_ADDR_V, phy_addr_y + Ysize);
+		} else {
+			enc_pr(LOG_ERROR, "not support fmt %d\n", info.fmt);
+		}
+
+	} else if (info.num_planes == 2) {
+		if (info.fmt == AMVENC_NV12 || info.fmt == AMVENC_NV21 ) {
+			WriteVpuRegister(W4_SRC_ADDR_U, phy_addr_u);
+			WriteVpuRegister(W4_SRC_ADDR_V, phy_addr_u);
+		} else {
+			enc_pr(LOG_ERROR, "not support fmt %d\n", info.fmt);
+		}
+
+	} else if (info.num_planes == 3) {
+		if (info.fmt == AMVENC_YUV420) {
+			WriteVpuRegister(W4_SRC_ADDR_U, phy_addr_u);
+			WriteVpuRegister(W4_SRC_ADDR_V, phy_addr_v);
+		} else {
+			enc_pr(LOG_ERROR, "not support fmt %d\n", info.fmt);
+		}
+	}
+	return 0;
+
+}
 
 static const struct file_operations vpu_fops = {
 	.owner = THIS_MODULE,
diff --git a/drivers/frame_sink/encoder/h265/vpu.h b/drivers/frame_sink/encoder/h265/vpu.h
index eaf764c..b89744f 100644
--- a/drivers/frame_sink/encoder/h265/vpu.h
+++ b/drivers/frame_sink/encoder/h265/vpu.h
@@ -23,6 +23,7 @@
 #include <linux/fs.h>
 #include <linux/types.h>
 #include <linux/compat.h>
+#include <linux/dma-buf.h>
 
 #define MAX_INST_HANDLE_SIZE	        (32*1024)
 #define MAX_NUM_INSTANCE                4
@@ -101,6 +102,21 @@
 	READ_HHI_REG(HHI_WAVE420L_CLK_CNTL2) \
 		& (~(1 << 8)))
 
+typedef enum
+{
+    AMVENC_YUV422_SINGLE = 0,
+    AMVENC_YUV444_SINGLE,
+    AMVENC_NV21,
+    AMVENC_NV12,
+    AMVENC_YUV420,
+    AMVENC_YUV444_PLANE,
+    AMVENC_RGB888,
+    AMVENC_RGB888_PLANE,
+    AMVENC_RGB565,
+    AMVENC_RGBA8888,
+    AMVENC_FRAME_FMT
+} AMVEncFrameFmt;
+
 #ifdef CONFIG_COMPAT
 struct compat_vpudrv_buffer_t {
 	u32 size;
@@ -162,6 +178,25 @@ struct vpudrv_instance_pool_t {
 	u8 codecInstPool[MAX_NUM_INSTANCE][MAX_INST_HANDLE_SIZE];
 };
 
+struct vpu_dma_buf_info_t {
+    u32 width;
+    u32 height;
+    AMVEncFrameFmt fmt;
+    u32 num_planes;
+    s32 fd[3];
+};
+
+struct vpu_dma_cfg {
+	int fd;
+	void *dev;
+	void *vaddr;
+	void *paddr;
+	struct dma_buf *dbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sg;
+	enum dma_data_direction dir;
+};
+
 #define VPUDRV_BUF_LEN struct vpudrv_buffer_t
 #define VPUDRV_BUF_LEN32 struct compat_vpudrv_buffer_t
 #define VPUDRV_INST_LEN struct vpudrv_inst_info_t
@@ -206,6 +241,12 @@ struct vpudrv_instance_pool_t {
 #define VDI_IOCTL_FLUSH_BUFFER \
 	_IOW(VDI_MAGIC, 13, VPUDRV_BUF_LEN)
 
+#define VDI_IOCTL_CONFIG_DMA \
+        _IOW(VDI_MAGIC, 14, struct vpu_dma_buf_info_t)
+
+#define VDI_IOCTL_UNMAP_DMA \
+        _IOW(VDI_MAGIC, 15, u32)
+
 #ifdef CONFIG_COMPAT
 #define VDI_IOCTL_ALLOCATE_PHYSICAL_MEMORY32 \
 	_IOW(VDI_MAGIC, 0, VPUDRV_BUF_LEN32)
@@ -294,6 +335,10 @@ enum {
 
 #define W4_BS_RD_PTR					(W4_REG_BASE + 0x0130)
 #define W4_BS_WR_PTR					(W4_REG_BASE + 0x0134)
+#define W4_SRC_ADDR_Y                                   (W4_REG_BASE + 0x0174)
+#define W4_SRC_ADDR_U                                   (W4_REG_BASE + 0x0178)
+#define W4_SRC_ADDR_V                                   (W4_REG_BASE + 0x017C)
+
 #define W4_RET_ENC_PIC_BYTE			(W4_REG_BASE + 0x01C8)
 
 #define W4_REMAP_CODE_INDEX			 0
